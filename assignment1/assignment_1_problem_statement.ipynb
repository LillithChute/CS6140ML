{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "assignment-1-problem-statement.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LillithChute/CS6140ML/blob/main/assignment1/assignment_1_problem_statement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af32m3yR69GW"
      },
      "source": [
        "# CS 6140 Machine Learning: Assignment - 1 (Total Points: 100)\n",
        "## Prof. Ahmad Uzair "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM-VKHN469Gb"
      },
      "source": [
        "### Q1. Classification Trees with numerical features (45 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV35tjz569Gb"
      },
      "source": [
        "### Datasets used for the problem:\n",
        "\n",
        "Iris: has three classes and the task is to accurately predict one of the three sub-types of the Iris flower given four different physical features. These features include the length and width of the sepals and the petals. There are a total of 150 instances with each class having 50 instances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PFR-77C69Gc"
      },
      "source": [
        "### Growing Decison Trees \n",
        "Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal of this question in the assignment is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. \n",
        "\n",
        "<i>Note: Write in your code only in the place holders where you are instructed to, replacing None.<i>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lf07c7UT69Gc"
      },
      "source": [
        "# Do not change the code in this cell\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib notebook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpP538dj69Gd"
      },
      "source": [
        "### Here is the first look at your dataset and its feature columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLWhGkJh69Gd"
      },
      "source": [
        "# Do not change the code in this cell\n",
        "iris_data = pd.read_csv(\"iris.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39y9fnpZ69Ge"
      },
      "source": [
        "# Do not change the code in this cell\n",
        "iris_data.drop(\"Id\", axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGcygNA069Ge"
      },
      "source": [
        "# Do not change the code in this cell\n",
        "iris_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFDsAfPe69Ge"
      },
      "source": [
        "# Do not change the code in this cell\n",
        "iris_data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VijmTxGW69Ge"
      },
      "source": [
        "# Do not change the code in this cell\n",
        "iris_data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gob86Ee69Gf"
      },
      "source": [
        "### Task\n",
        "Shuffle the data and change the categorical features mentioned in the species column to numeric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRbD78rP69Gf"
      },
      "source": [
        "# Start code here\n",
        "# Replace the categorical target values in the Species column to numeric\n",
        "\n",
        "# Shuffle the data\n",
        "iris_data = None\n",
        "# End code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSxpjuzx69Gf"
      },
      "source": [
        "# Do not change the code in this cell\n",
        "iris_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s2FrXw369Gf"
      },
      "source": [
        "### Task\n",
        "Time to code your decision tree.\n",
        "\n",
        "In the following cell, create a node class for your Decision Tree Classifier having the following attributes:\n",
        "feature_index, threshold, left, right, info_gain, value, where the condition upon which the decision will be based would be defined by feature_index and threshold, while the attributes left and right will be for accessing the left and the right child of a particular node other than the leaf node in the decision tree. info_gain will denote the information gained by the split denoted by the particular decision node. The value attribute will be holding the value of the majority class of the leaf node without having the other attributes. This will help us to determine the class of new data point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubEqF8IH69Gg"
      },
      "source": [
        "class Node:  \n",
        "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, info_gain=None, value=None):\n",
        "        # Start code here\n",
        "        # for decision node\n",
        "        self.feature_index = None\n",
        "        self.threshold = None\n",
        "        self.left = None\n",
        "        self.right = None\n",
        "        self.info_gain = None\n",
        "        \n",
        "        # for leaf node\n",
        "        self.value = None\n",
        "    # End code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSQwIQDi69Gg"
      },
      "source": [
        "### Task\n",
        "In the following cell, you will create a Decision Tree Classifier from scratch class having the following attributes: root, min_samples_split, max_depth. Other instructions have been given in doc strings and comments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VIOuiPP69Gg"
      },
      "source": [
        "class DecisionTreeClassifier:\n",
        "    def __init__(self, min_samples_split=2, max_depth=2):\n",
        "        # Start code here\n",
        "        # Initialize the root of the decision tree to traverse through the decision tree to None\n",
        "        self.root = None\n",
        "        # initialize the stopping conditions\n",
        "        self.min_samples_split = None\n",
        "        self.max_depth = None\n",
        "        # End code here \n",
        "        \n",
        "        \n",
        "    def build_tree(self, dataset, curr_depth = 0):\n",
        "        \"\"\"\n",
        "        This will be a recursive function to build the decision tree.\n",
        "        dataset: The data that you will be using for your assignment\n",
        "        curr_depth: Current depth of the tree\n",
        "        Returns the leaf node\n",
        "        \"\"\"\n",
        "        # Start code here\n",
        "        # Separate the features and targets into two variables X and Y\n",
        "        X, Y = None\n",
        "        # Extract the number of samples and number of features\n",
        "        num_samples, num_features = None\n",
        "        \n",
        "        # split until stopping conditions are met\n",
        "        if num_samples >= self.min_samples_split and curr_depth <= self.max_depth:\n",
        "            # finding the best split\n",
        "            best_split = None\n",
        "            # check if information gain is positive\n",
        "            if best_split[\"info_gain\"] > 0:\n",
        "                # recur left\n",
        "                left_subtree = None\n",
        "                # recur right\n",
        "                right_subtree = None\n",
        "                # return the decision node in the form of a dictionary\n",
        "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"],\n",
        "                           left_subtree, right_subtree, best_split[\"info_gain\"])\n",
        "        # compute leaf node\n",
        "        leaf_value = self.calculate_leaf_value(Y)\n",
        "        # End code here\n",
        "        return Node(value=leaf_value)\n",
        "    \n",
        "        \n",
        "    def get_best_split(self, dataset, num_samples, num_features):\n",
        "        \"\"\"\n",
        "        Function to find out the best split\n",
        "        dataset: input data\n",
        "        num_samples: Number of samples present in the dataset\n",
        "        num_features: Number of features in the dataset\n",
        "        Returns the best split\n",
        "        \"\"\"\n",
        "        \n",
        "        # dictionary to store the best split\n",
        "        best_split = {}\n",
        "        max_info_gain = -float(\"inf\")\n",
        "        \n",
        "        # Start code here\n",
        "        # loop over all the features in the data\n",
        "        for feature_index in range(num_features):\n",
        "            feature_values = None\n",
        "            # Hint: You can use np.unique function to retrieve the values of the possible threshold\n",
        "            possible_thresholds = None\n",
        "            # loop over all the feature values present in the data\n",
        "            for threshold in possible_thresholds:\n",
        "                # get current split\n",
        "                dataset_left, dataset_right = None\n",
        "                # check if children are not null\n",
        "                if len(dataset_left) > 0 and len(dataset_right) > 0:\n",
        "                    y, left_y, right_y = None\n",
        "                    # compute information gain\n",
        "                    curr_info_gain = None\n",
        "                    # update the best split if needed\n",
        "                    if curr_info_gain > max_info_gain:\n",
        "                        best_split[\"feature_index\"] = None\n",
        "                        best_split[\"threshold\"] = None\n",
        "                        best_split[\"dataset_left\"] = None\n",
        "                        best_split[\"dataset_right\"] = None\n",
        "                        best_split[\"info_gain\"] = None\n",
        "                        max_info_gain = None\n",
        "        # End code here\n",
        "\n",
        "        return best_split\n",
        "                    \n",
        "                    \n",
        "    def split(self, dataset, feature_index, threshold):\n",
        "        \"\"\"\n",
        "        Function to split the data to the left child and right child in the decision tree\n",
        "        dataset: input data\n",
        "        feature_index: feature index used to locate the index of the feature in a particular row in the dataset\n",
        "        threshold: threshold value based on which the split will be calculated\n",
        "        Returns the left and right datavalues from the dataset\n",
        "        \"\"\"\n",
        "        # Start code here\n",
        "        # Hint: Use list comprehension to distinguish which values would be present in left and right \n",
        "        # subtree on the basis of threshold\n",
        "        dataset_left = None\n",
        "        dataset_right = None\n",
        "        # End code here\n",
        "        return dataset_left, dataset_right\n",
        "        \n",
        "        \n",
        "    def information_gain(self, parent, l_child, r_child, mode=\"entropy\"):\n",
        "        \"\"\"\n",
        "        Function to calculate information gain. This function subtracts the combined information \n",
        "        of the child node from the parent node.\n",
        "        parent: value of parent node\n",
        "        l_child: value of left child node\n",
        "        r_child: value of right child node\n",
        "        mode: based on which information gain will be calculated either entropy/gini index\n",
        "        Returns the information gain\n",
        "        \"\"\"\n",
        "        # Start code here\n",
        "        # Calculate the relative sizes of the child node with respect to the parent node\n",
        "        weight_l = None\n",
        "        weight_r = None\n",
        "        # Calculate gain on the with respect to the information gain parameter which will either be \n",
        "        # gini_index or entropy\n",
        "        if mode == \"gini\":\n",
        "            gain = None\n",
        "        else:\n",
        "            gain = None\n",
        "        # End code here\n",
        "        return gain\n",
        "    \n",
        "    \n",
        "    def entropy(self, y):\n",
        "        \"\"\"\n",
        "        Function to calculate the entropy\n",
        "        y: target labels\n",
        "        Returns entropy\n",
        "        \"\"\"\n",
        "        # Start code here\n",
        "        # Extract the class labels\n",
        "        class_labels = None\n",
        "        # Initialize the entropy\n",
        "        entropy = None\n",
        "        # Calculate the entropy\n",
        "        for cls in class_labels:\n",
        "            p_cls = None\n",
        "            entropy = None\n",
        "        # End code here\n",
        "        return entropy\n",
        "    \n",
        "    \n",
        "    def gini_index(self, y):\n",
        "        \"\"\"\n",
        "        Function to calculate gini index\n",
        "        y: target labels\n",
        "        Returns gini index\n",
        "        \"\"\"\n",
        "        # Extract the class labels\n",
        "        class_labels = np.unique(y)\n",
        "        # Initialize the gini_index\n",
        "        gini = 0\n",
        "        # Calculate the gini index\n",
        "        for cls in class_labels:\n",
        "            p_cls = len(y[y==cls]) / len(y)\n",
        "            gini += p_cls ** 2\n",
        "        return 1 - gini\n",
        "    \n",
        "    \n",
        "    def calculate_leaf_value(self, Y):\n",
        "        \"\"\"\n",
        "        Function to compute thr value of leaf node\n",
        "        Y: target labels\n",
        "        Returns leaf node value\n",
        "        \"\"\"\n",
        "        # Start code here\n",
        "        # Return the most occuring element in Y. Hint: you can use lists \n",
        "        Y = None\n",
        "        return None\n",
        "        # End code here\n",
        "    \n",
        "    def print_tree(self, tree = None, indent = \" \"):\n",
        "        \"\"\"\n",
        "        Function to print the tree. Use the pre-order traversal method to print the decision tree.\n",
        "        # Do not make any changes in this function\n",
        "        \"\"\"\n",
        "        \n",
        "        if not tree:\n",
        "            tree = self.root\n",
        "        \n",
        "        if tree.value is not None:\n",
        "            print(tree.value)\n",
        "            \n",
        "        else:\n",
        "            print(\"X \" + str(tree.feature_index), \"<=\", tree.threshold, \"?\", tree.info_gain)\n",
        "            print(\"%sleft:\" % (indent), end = \" \")\n",
        "            self.print_tree(tree.left, indent + indent)\n",
        "            print(\"%sright\" % (indent), end = \" \")\n",
        "            self.print_tree(tree.right, indent + indent)\n",
        "            \n",
        "            \n",
        "    def fit(self, X, Y):\n",
        "        \"\"\"\n",
        "        Function to train the tree.\n",
        "        X: Features\n",
        "        Y: Target\n",
        "        \"\"\"\n",
        "        # Start code here\n",
        "        # Concatenate X, Y to create the dataset and call the build_tree function recursively\n",
        "        dataset = None\n",
        "        self.root = None\n",
        "        # End code here\n",
        "        \n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Prediction function to calculate the all the predictions of the matrix of features \n",
        "        provided using make_predictions function\n",
        "        X: Matrix of features\n",
        "        Returns predictions using the make_predictions function\n",
        "        \"\"\"\n",
        "        # Start code here\n",
        "        predictions = None\n",
        "        # End code here\n",
        "        return predictions\n",
        "    \n",
        "    \n",
        "    def make_predictions(self, x, tree):\n",
        "        \"\"\"\n",
        "        Function to predict a single datapoint\n",
        "        x: data\n",
        "        tree: current tree\n",
        "        Returns predictions\n",
        "        \"\"\"\n",
        "        # Start code here\n",
        "        # return the value if the node is a leaf node\n",
        "        if tree.value != None:\n",
        "            return None\n",
        "        # Extract feature values of a new datapoint at a given feature index\n",
        "        feature_val = None\n",
        "        # Recur through left or right subtree \n",
        "        None\n",
        "      # End code here\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2-F9yXJ69Gh"
      },
      "source": [
        "### Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hcGYXol69Gh"
      },
      "source": [
        "# Do not change the code in this cell\n",
        "X = iris_data.iloc[:, :-1].values\n",
        "Y = iris_data.iloc[:, -1].values.reshape(-1, 1)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 41)\n",
        "classifier = DecisionTreeClassifier(min_samples_split=3, max_depth=3)\n",
        "classifier.fit(X_train, Y_train)\n",
        "classifier.print_tree()\n",
        "y_pred = classifier.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy is: \"+str(accuracy_score(Y_test, y_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaLr83zN69Gh"
      },
      "source": [
        "### Q2. Regression (35 points)\n",
        "Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable, and the other is considered to be a dependent variable. \n",
        "\n",
        "Ridge regression is a method of estimating the coefficients of multiple-regression models in scenarios where independent variables are highly correlated. \n",
        "\n",
        "Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. The idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent.\n",
        "\n",
        "The goal of this question in the assignment is to create a model that predicts the value of a target variable by learning the relationship between independent and its corresponding dependent variable using linear regression, ridge regression and gradient descent.  \n",
        "\n",
        "<i>Note: Write in your code only in the place holders where you are instructed to, replacing None.<i>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQcs_Exb69Gh"
      },
      "source": [
        "## Gradient descent algorithm \n",
        "\\begin{equation}\n",
        "\\theta^{+} = \\theta^{-} + \\frac{\\alpha}{m} (y_{i} - h(x_{i}) )\\bar{x}\n",
        "\\end{equation}\n",
        "\n",
        "This minimizes the following cost function\n",
        "\n",
        "\\begin{equation}\n",
        "J(x, \\theta, y) = \\frac{1}{2m}\\sum_{i=1}^{m}(h(x_i) - y_i)^2\n",
        "\\end{equation}\n",
        "\n",
        "where\n",
        "\\begin{equation}\n",
        "h(x_i) = \\theta^T \\bar{x}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut5WI0bf69Gi"
      },
      "source": [
        "# Do not change the code in this cell\n",
        "true_slope = 15\n",
        "true_intercept = 2.4\n",
        "input_var = np.arange(0.0,100.0)\n",
        "output_var = true_slope * input_var + true_intercept + 300.0 * np.random.rand(len(input_var))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxlwqyGa69Gi"
      },
      "source": [
        "# Do not change the code in this cell\n",
        "plt.figure()\n",
        "plt.scatter(input_var, output_var)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsgvYPmF69Gi"
      },
      "source": [
        "def compute_cost(ip, op, params):\n",
        "    \"\"\"\n",
        "    Cost function in linear regression where the cost is calculated\n",
        "    ip: input variables\n",
        "    op: output variables\n",
        "    params: corresponding parameters\n",
        "    Returns cost\n",
        "    \"\"\"\n",
        "    # Start code here\n",
        "    num_samples = None\n",
        "    cost_sum = None\n",
        "    for x,y in zip(ip, op):\n",
        "        y_hat = None\n",
        "        cost_sum = None\n",
        "    \n",
        "    cost = None\n",
        "    # End code here\n",
        "    return cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FBliO5B69Gi"
      },
      "source": [
        "\n",
        "### Batch gradient descent\n",
        "Algorithm can be given as follows:\n",
        "\n",
        "```for j in 0 -> max_iteration: \n",
        "    for i in 0 -> m: \n",
        "        theta += (alpha / m) * (y[i] - h(x[i])) * x_bar\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdQiC_zD69Gj"
      },
      "source": [
        "def linear_regression_using_batch_gradient_descent(ip, op, params, alpha, max_iter):\n",
        "    \"\"\"\n",
        "    Compute the params for linear regression using batch gradient descent\n",
        "    ip: input variables\n",
        "    op: output variables\n",
        "    params: corresponding parameters\n",
        "    alpha: learning rate\n",
        "    max_iter: maximum number of iterations\n",
        "    Returns parameters, cost, params_store\n",
        "    \"\"\" \n",
        "    # Start code here\n",
        "    # initialize iteration, number of samples, cost and parameter array\n",
        "    iteration = None\n",
        "    num_samples = None\n",
        "    cost = None\n",
        "    params_store = None\n",
        "    \n",
        "    # Compute the cost and store the params for the corresponding cost\n",
        "    while iteration < max_iter:\n",
        "        cost[iteration] = None\n",
        "        params_store[:, iteration] = None\n",
        "        \n",
        "        print('--------------------------')\n",
        "        print(f'iteration: {iteration}')\n",
        "        print(f'cost: {cost[iteration]}')\n",
        "        \n",
        "        # Apply batch gradient descent\n",
        "        for x,y in zip(ip, op):\n",
        "            y_hat = None\n",
        "            gradient = None\n",
        "            params = None\n",
        "            \n",
        "        iteration = None\n",
        "    # End code here\n",
        "    \n",
        "    return params, cost, params_store"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f27dbWG069Gj"
      },
      "source": [
        "# Do not change the code in this cell\n",
        "# Training the model\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(input_var, output_var, test_size=0.20)\n",
        "\n",
        "params_0 = np.array([20.0, 80.0])\n",
        "\n",
        "alpha_batch = 1e-3\n",
        "max_iter = 500\n",
        "params_hat_batch, cost_batch, params_store_batch =\\\n",
        "    linear_regression_using_batch_gradient_descent(x_train, y_train, params_0, alpha_batch, max_iter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iEtNAZR69Gj"
      },
      "source": [
        "### Stochastic Gradient Descent\n",
        "Algorithm can be given as follows:\n",
        "```shuffle(x, y)\n",
        "for i in 0 -> m:\n",
        "    theta += (alpha / m) * (y[i] - h(x[i])) * x_bar  \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhBJ56xx69Gj"
      },
      "source": [
        "def linear_regression_using_stochastic_gradient_descent(ip, op, params, alpha):\n",
        "    \"\"\"\n",
        "    Compute the params for linear regression using stochastic gradient descent\n",
        "    ip: input variables\n",
        "    op: output variables\n",
        "    params: corresponding parameters\n",
        "    alpha: learning rate\n",
        "    Returns parameters, cost, params_store\n",
        "    \"\"\"\n",
        "    # Start code here\n",
        "    # initialize iteration, number of samples, cost and parameter array\n",
        "    num_samples = None\n",
        "    cost = None\n",
        "    params_store = None\n",
        "    \n",
        "    i = 0\n",
        "    # Compute the cost and store the params for the corresponding cost\n",
        "    for x,y in zip(input_var, output_var):\n",
        "        cost[i] = None\n",
        "        params_store[:, i] = None\n",
        "        \n",
        "        print('--------------------------')\n",
        "        print(f'iteration: {i}')\n",
        "        print(f'cost: {cost[i]}')\n",
        "        \n",
        "        # Apply stochastic gradient descent\n",
        "        y_hat = None\n",
        "        gradient = None\n",
        "        params = None\n",
        "        \n",
        "        i = None\n",
        "            \n",
        "    return params, cost, params_store"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UFfsb6o69Gk"
      },
      "source": [
        "# Do not change the code in this cell\n",
        "alpha = 1e-3\n",
        "params_0 = np.array([20.0, 80.0])\n",
        "params_hat, cost, params_store =\\\n",
        "linear_regression_using_stochastic_gradient_descent(x_train, y_train, params_0, alpha)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBujNGC669Gk"
      },
      "source": [
        "plt.figure()\n",
        "plt.scatter(x_test, y_test)\n",
        "plt.plot(x_test, params_hat_batch[0] + params_hat_batch[1]*x_test, 'g', label='batch')\n",
        "plt.plot(x_test, params_hat[0] + params_hat[1]*x_test, '-r', label='stochastic')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "print(f'Batch      T0, T1: {params_hat_batch[0]}, {params_hat_batch[1]}')\n",
        "print(f'Stochastic T0, T1: {params_hat[0]}, {params_hat[1]}')\n",
        "# Calculate Root Mean Square error in batch gradient descent algorithm and stochastic gradient descent algorithm\n",
        "rms_batch = None\n",
        "rms_stochastic = None\n",
        "print(f'Batch RMS:      {rms_batch}')\n",
        "print(f'Stochastic RMS: {rms_stochastic}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLeMQez069Gk"
      },
      "source": [
        "# Do not change the code in this cell\n",
        "plt.figure()\n",
        "plt.plot(np.arange(max_iter), cost_batch, 'r', label='batch')\n",
        "plt.plot(np.arange(len(cost)), cost, 'g', label='stochastic')\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('normalized cost')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "print(f'min cost with BGD: {np.min(cost_batch)}')\n",
        "print(f'min cost with SGD: {np.min(cost)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z3JsbCS69Gk"
      },
      "source": [
        "### Ridge Regression\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{\\theta} = (X^TX + \\lambda I)^{-1}X^TY\n",
        "\\end{equation}\n",
        "\n",
        "where\n",
        "\\begin{equation}\n",
        "X = [\\bar{x}^T_1, \\bar{x}^T_2, ... , \\bar{x}^T_n]^T\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "Y = [y_1, y_2, ... , y_n]^T\n",
        "\\end{equation}\n",
        "\n",
        "This solution minimizes the following cost function\n",
        "\n",
        "\\begin{equation}\n",
        "J(x, \\theta, y) = \\sum_{i=1}^{m}(\\theta^T\\bar{x}_i - y_i)^2 + \\lambda ||\\theta||^2\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Byo4Vxer69Gk"
      },
      "source": [
        "class Ridge:\n",
        "    \"\"\"\n",
        "    Linear least squares with L2 regularization.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, lam):\n",
        "        \"\"\"\n",
        "        Initialize a Ridge object.\n",
        "        lam: the regularization factor \n",
        "        \"\"\"\n",
        "        self._lambda = lam\n",
        "        \n",
        "    @staticmethod\n",
        "    def _x_bar(x):\n",
        "        \"\"\"\n",
        "        Create the vector x_bar.\n",
        "        x: input vector\n",
        "        \"\"\"\n",
        "        # Start code here\n",
        "        return None\n",
        "        # End code here\n",
        "    \n",
        "    def fit(self, x_train, y_train):\n",
        "        \"\"\"\n",
        "        Generate a fit for the data.\n",
        "        x_train: the input values of the training data\n",
        "        y_train: the output values of the training data\n",
        "        \"\"\"\n",
        "        # Start code here\n",
        "        # stack the data\n",
        "        X = None\n",
        "        Y = None\n",
        "        \n",
        "        # compute the model coeff\n",
        "        XT = None\n",
        "        XTX = None\n",
        "        self._coeff_hat = None\n",
        "        # End code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxuXKaOd69Gl"
      },
      "source": [
        "# Do not change the code in this cell\n",
        "c2 = 0.03\n",
        "c1 = 1.25\n",
        "c0 = 3.23\n",
        "x_in = np.linspace(-15.0, 19.4, 250)\n",
        "y_out = c1 * x_in ** 2 + c1 * x_in + c0 + 400.0 * np.random.rand(len(x_in))\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(x_in, y_out)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwE75l3M69Gl"
      },
      "source": [
        "# Do not change the code in this cell\n",
        "# Train using the custom Ridge class\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x_in, y_out, test_size=0.20)\n",
        "\n",
        "lam = 0.1\n",
        "ridge = Ridge(lam)\n",
        "ridge.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kQ4gQjC69Gl"
      },
      "source": [
        "# Do not change the code in this cell\n",
        "# Plot test data and model predictions\n",
        "plt.figure()\n",
        "plt.scatter(x_test, y_test)\n",
        "x_test_sorted = np.sort(x_test)\n",
        "plt.plot(x_test_sorted,\n",
        "         ridge._coeff_hat[0] + ridge._coeff_hat[1]*x_test_sorted + ridge._coeff_hat[2]*x_test_sorted**2,\n",
        "         '-r', label='custom')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# print the coeff\n",
        "print(f'custom: {ridge._coeff_hat[0]}, {ridge._coeff_hat[1]}, {ridge._coeff_hat[2]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxj32cLk69Gl"
      },
      "source": [
        "# Effect of regularization factor\n",
        "coeff_store = []\n",
        "norm_store = []\n",
        "factors = np.linspace(0.0, 1.0, 10)\n",
        "for l in factors:\n",
        "    # Star code here\n",
        "    # instantiate the Ridge class using l and store it in ridge\n",
        "    ridge = None\n",
        "    # fit the data to your model\n",
        "    None\n",
        "    # append co-effecients \n",
        "    None\n",
        "    # append normalized co-effecients using np.linalg.norm\n",
        "    None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suvJJ1bm69Gl"
      },
      "source": [
        "# Do not change the code in this cell\n",
        "plt.figure()\n",
        "plt.subplot(411)\n",
        "coeff_0 = [c[0] for c in coeff_store]\n",
        "plt.plot(factors, coeff_0, 'or')\n",
        "plt.ylabel('c0')\n",
        "plt.subplot(412)\n",
        "coeff_1 = [c[1] for c in coeff_store]\n",
        "plt.plot(factors, coeff_1, 'og')\n",
        "plt.ylabel('c1')\n",
        "plt.subplot(413)\n",
        "coeff_2 = [c[2] for c in coeff_store]\n",
        "plt.plot(factors, coeff_2, 'ob')\n",
        "plt.ylabel('c2')\n",
        "plt.subplot(414)\n",
        "plt.plot(factors, norm_store, 'o')\n",
        "plt.xlabel('lambda')\n",
        "plt.ylabel('Norm')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eO5huYp69Gl"
      },
      "source": [
        "### Q3. Theory (10 marks)\n",
        "\n",
        "Consider training a binary decision tree using entropy splits.\n",
        "\n",
        "(a) Prove that the decrease in entropy by a split on a binary yes/no feature can never be greater than 1 bit.\n",
        "\n",
        "(b) Generalize this result to the case of arbitrary multiway branching."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfiFkG7o69Gm"
      },
      "source": [
        "### Q4. Gradient Descent Application (10 marks)\n",
        "\n",
        "1. For the function \\begin{equation} f(x, y) = ln(1 + xy), \\end{equation}\n",
        "find the unit vector that gives the direction of steepest descent at the point\n",
        "P (2, 3). Also find the direction of no change at this point.\n",
        "\n",
        "\n",
        "2. A businessperson can sell a product in France and Japan and charge different prices in each region. Let x represent the number of units sold in France and y represent the number of units sold in Japan. According to demand rules, the business owner must set the unit price at 97-(x/10) dollars in France and 83-(y/20) dollars in Japan in order to sell all of the units. The cost of production is ${\\$ 3}$ per unit, in addition to a base capital of ${\\$ 20,000}.$ If the industrialist intends to maximize profit, how many units should he aim to sell in each country?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56nI1-gi69Gm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}